from collections.abc import Iterable
from typing import Literal

from pydantic import BaseModel, ConfigDict, Field


class ChatCompletionContentPartParam(BaseModel):
    """Content part parameter for messages."""

    text: str
    type: Literal["text"]


class FunctionCall(BaseModel):
    """Function call parameters."""

    arguments: str = Field(
        ...,
        description="The arguments to call the function with, as generated by the model in JSON format.",
    )
    name: str = Field(..., description="The name of the function to call.")
    model_config = ConfigDict(extra="ignore")


class ToolCallParam(BaseModel):
    """Tool call parameter."""

    id: str
    function: FunctionCall
    type: Literal["function"]
    model_config = ConfigDict(extra="ignore")


class ChatCompletionMessageParam(BaseModel):
    """A chat completion message parameter."""

    content: str | Iterable[ChatCompletionContentPartParam] | None = Field(default=None)
    role: Literal["user", "assistant", "system", "tool", "developer"]
    name: str | None = Field(
        default=None, description="An optional name for the participant."
    )
    tool_call_id: str | None = None
    tool_calls: Iterable[ToolCallParam] | None = None
    model_config = ConfigDict(extra="ignore")


class FunctionDefinition(BaseModel):
    """Function definition for tools."""

    name: str = Field(
        ...,
        description="The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.",
    )
    description: str = Field(
        ...,
        description="A description of what the function does, used by the model to choose when and how to call the function.",
    )
    parameters: dict[str, object] = Field(
        ...,
        description="The parameters the functions accepts, described as a JSON Schema object.",
    )
    strict: bool | None = Field(
        default=None,
        description="Whether to enable strict schema adherence when generating the function call.",
    )
    model_config = ConfigDict(extra="ignore")


class ChatCompletionToolParam(BaseModel):
    """Tool parameter for chat completion."""

    function: FunctionDefinition
    type: Literal["function"]
    model_config = ConfigDict(extra="ignore")


class ChatCompletionRequest(BaseModel):
    """
    Chat completion request that aligns with the latest OpenAI API.
    Based on openai.resources.chat.completions.completions.Completions.create() method.
    """

    # Required parameters
    messages: Iterable[ChatCompletionMessageParam] = Field(
        ..., description="A list of messages comprising the conversation so far."
    )
    model: str = Field(
        ...,
        description="Model ID used to generate the response, like 'gpt-4o,openai/gpt-5-mini' or 'openai/gpt-5-mini'.",
    )
    # Optional parameters
    frequency_penalty: float | None = Field(
        default=None,
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency.",
    )
    function_call: str | dict | None = Field(
        default=None,
        description="Deprecated in favor of tool_choice. Controls which (if any) function is called by the model.",
    )
    functions: Iterable[dict] | None = Field(
        default=None,
        description="Deprecated in favor of tools. A list of functions the model may generate JSON inputs for.",
    )
    logit_bias: dict[str, int] | None = Field(
        default=None,
        description="Modify the likelihood of specified tokens appearing in the completion.",
    )
    logprobs: bool | None = Field(
        default=None,
        description="Whether to return log probabilities of the output tokens or not.",
    )
    max_completion_tokens: int | None = Field(
        default=None,
        description="An upper bound for the number of tokens that can be generated for a completion.",
    )
    max_tokens: int | None = Field(
        default=None,
        description="Deprecated in favor of max_completion_tokens. The maximum number of tokens that can be generated.",
    )
    modalities: list[Literal["text", "audio"]] | None = Field(
        default=None,
        description="Output types that you would like the model to generate.",
    )
    parallel_tool_calls: bool | None = Field(
        default=None,
        description="Whether to enable parallel function calling during tool use.",
    )
    presence_penalty: float | None = Field(
        default=None,
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far.",
    )
    reasoning_effort: Literal["minimal", "low", "medium", "high"] | None = Field(
        default=None, description="Constrains effort on reasoning for reasoning models."
    )
    seed: int | None = Field(
        default=None,
        description="If specified, our system will make a best effort to sample deterministically.",
    )
    stop: str | list[str] | None = Field(
        default=None,
        description="Up to 4 sequences where the API will stop generating further tokens.",
    )
    store: bool | None = Field(
        default=None,
        description="Whether or not to store the output of this chat completion request for use in model distillation or evals.",
    )
    stream: bool | None = Field(
        default=None,
        description="If set to true, the model response data will be streamed to the client as it is generated.",
    )
    temperature: float | None = Field(
        default=None, description="What sampling temperature to use, between 0 and 2."
    )
    tools: list[ChatCompletionToolParam] | None = Field(
        default=None,
        description="A list of tools the model may call. Currently, only functions are supported as a tool.",
    )
    top_logprobs: int | None = Field(
        default=None,
        description="An integer between 0 and 20 specifying the number of most likely tokens to return at each token position.",
    )
    top_p: float | None = Field(
        default=None,
        description="An alternative to sampling with temperature, called nucleus sampling.",
    )
    verbosity: Literal["low", "medium", "high"] | None = Field(
        default=None, description="Constrains the verbosity of the model's response."
    )

    session_id: str | None = Field(
        default=None,
        description="An optional session ID to associate with the request for tracking tool calls.",
    )
