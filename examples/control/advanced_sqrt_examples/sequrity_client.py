# %% [markdown]
# Friends, we are very excited to announce **AI Sequrity Control** ‚Äì- our first beta product to deploy DualLLM architecture for your AI instantiation with no headaches. **Sequrity Control** is not just yet another guardrail system ‚Äì- it gives you much stronger guarantees and enables you to reason formally about requirements of your agent covering tool-use security policies and also even business logic.
#
# Most standard AI setups use a single, powerful LLM to both understand your instructions and interact with your data. This is the simplest approach, but it's also the most vulnerable. This mixing of control-flow (the instructions to execute) and untrusted data is the root cause of prompt injection vulnerabilities. For instance, a malicious instruction hidden in a calendar invite could trick the AI into leaking sensitive commercial information.
#
# At [**sequrity.ai**](https://sequrity.ai/), we believe in a fundamentally different and more secure design. Instead of patching a broken model, we have re-architected the entire process with our Dual-LLM, "Plan-Then-Execute" architecture. This approach creates a formal separation between understanding instructions and acting on data, providing architectural guarantees that other solutions simply can't match.
#
# Here's how it works:
#
# - The Planner LLM: This first model reads your prompt and creates a safe, step-by-step plan.
# - The Security Checkpoint: This is our crucial innovation. This engine intercepts and analyzes the plan before any action is taken. It validates the plan against your defined policies, ensuring that malicious instructions found in your data can never become actions.
# - Secure Execution: Only the vetted, secure steps of the plan are executed. Malicious instructions are correctly identified as data and ignored, preventing the instruction from ever crossing the boundary from data flow to control flow.
#
# Below, we give a number of examples of where Sequrity Control shines.
#
# **Stop Patching, Start Building on a Foundation of Certainty**
#
# Sequrity Control offers a deterministic approach that makes the system's behavior predictable and auditable. It becomes architecturally impossible for the AI to violate your security policy. This fundamentally shifts liability from the unpredictable nature of an AI to the accountable decisions of a system user.
#
# Our system enables you to enforce powerful, fine-grained policies. A policy isn't just about what role can use a certain tool; it's about defining specific conditions, such as allowing an action only if the user is a manager, the data is not personally identifiable information, and it's within work hours. This is the difference between a bouncer checking an ID at the door and a security guard monitoring actions inside.
#
#  **Below we provide special use-cases where you can already use Control**:
#
#

# %% [markdown]
# ## Setup & Helper Functions
#
# This example uses the **SequrityClient** from the `sequrity` package instead of raw REST API calls.
# The client provides a cleaner, type-safe interface for interacting with Sequrity Control.

# %%
# @title Settling the keys and the endpoint
import json
import os
import re
from typing import Callable, Literal

from rich.console import Console
from rich.syntax import Syntax

from sequrity import SequrityClient
from sequrity.control import (
    ControlFlowMetaPolicy,
    FeaturesHeader,
    FineGrainedConfigHeader,
    FsmOverrides,
    InternalPolicyPresets,
    SecurityPolicyHeader,
)
from sequrity.control.types.headers import ResponseFormatOverrides

# Client configuration
open_router_key = "your OpenRouter/OAI key"  # @param {type: "string"}
sequrity_key = "your SequrityAI key"  # @param {type: "string"}

CONFIG = {
    "open_router_api_key": os.getenv("OPENROUTER_API_KEY", open_router_key),
    "sequrity_key": os.getenv("SEQURITY_API_KEY", sequrity_key),
    "sequrity_base_url": os.getenv("SEQURITY_BASE_URL", None),
}

assert CONFIG["open_router_api_key"] != "your OpenRouter/OAI key"
assert CONFIG["sequrity_key"] != "your SequrityAI key"

# Initialize the Sequrity client
client = SequrityClient(api_key=CONFIG["sequrity_key"], timeout=120, base_url=CONFIG["sequrity_base_url"])

# %% [markdown]
# ### Mock client using SequrityClient
#
# This version uses the SequrityClient instead of raw HTTP requests.
# The client handles header construction and authentication automatically.

# %%


# --8<-- [start:run_workflow]
def t_print(s, max_len: int | None = 240):
    # trim long strings for printing
    s = str(s)
    if max_len is not None and len(s) > max_len:
        s = s[: max_len - 3] + "..."
    print(s)


def run_workflow(
    model: str,
    messages: list[dict],
    tool_defs: list[dict],
    tool_map: dict[str, Callable],
    features: FeaturesHeader | None,
    security_policy: SecurityPolicyHeader | None,
    fine_grained_config: FineGrainedConfigHeader | None,
    reasoning_effort: str = "minimal",
) -> tuple[Literal["success", "denied by policies", "unexpected error"], list[dict]]:
    interaction_id = 1
    console = Console()

    while True:
        print(f"\t--- Interaction {interaction_id} ---")
        response = send_request_to_endpoint(
            model=model,
            messages=messages,
            tool_defs=tool_defs,
            features=features,
            security_policy=security_policy,
            fine_grained_config=fine_grained_config,
            reasoning_effort=reasoning_effort,
        )

        if response is None:
            print("No response received, terminating workflow.")
            return "unexpected error", messages

        # append assistant message
        messages.append(response.choices[0].message.model_dump(exclude_none=True))

        finish_reason = response.choices[0].finish_reason
        if finish_reason == "stop":
            content = response.choices[0].message.content
            details = json.loads(content)
            if "program" in details:
                print("\nExecuted program:")
                syntax = Syntax(details["program"], "python", theme="github-dark", line_numbers=True)
                console.print(syntax)

            if details["status"] == "failure":
                if (
                    "denied by argument checking policies" in content
                    or "program violated control flow policies" in content
                ):
                    t_print(f"\tüö® Request denied by policies:\n\t{details['error']['message']}")
                    return "denied by policies", messages
                elif '"denied": [{' in content:
                    t_print(f"\tüö® Request denied by policies:\n\t{details['policy_check_history']}")
                    return "denied by policies", messages
                else:
                    t_print(f"\t‚ùå Request failed due to error:\n\t{details['error']['message']}")
                    return "unexpected error", messages
            else:
                # status == "success"
                t_print(f"‚òëÔ∏è Final response: {content}")
                return "success", messages
        elif finish_reason == "tool_calls":
            tool_calls = response.choices[0].message.tool_calls
            for tool_call in tool_calls:
                tool_name = tool_call.function.name
                tool_args = json.loads(tool_call.function.arguments)

                if tool_name in tool_map:
                    fn = tool_map[tool_name]
                    tool_result = fn(**tool_args)
                    t_print(f"\tüõ†Ô∏è Executed tool '{tool_name}' with args {tool_args}")
                    messages.append(
                        {
                            "role": "tool",
                            "content": tool_result,
                            "tool_call_id": tool_call.id,
                        }
                    )
                else:
                    print(f"\t‚õî Tool '{tool_name}' not found in tool map.")
                    return "unexpected error", messages
        else:
            print(f"\t‚õî Unknown finish reason: {finish_reason}, terminating workflow.")
            return "unexpected error", messages
        interaction_id += 1


# --8<-- [end:run_workflow]


# --8<-- [start:send_request_to_endpoint]
def send_request_to_endpoint(
    model: str,
    messages: list[dict],
    tool_defs: list[dict],
    features: FeaturesHeader | None,
    security_policy: SecurityPolicyHeader | None,
    fine_grained_config: FineGrainedConfigHeader | None,
    reasoning_effort: str = "minimal",
    session_id: str | None = None,
):
    try:
        response = client.control.chat.create(
            messages=messages,
            model=model,
            llm_api_key=CONFIG["open_router_api_key"],
            features=features,
            security_policy=security_policy,
            fine_grained_config=fine_grained_config,
            provider="openrouter",
            reasoning_effort=reasoning_effort,
            tools=tool_defs,
            session_id=session_id,
        )
        return response
    except Exception as e:
        print(f"API Request failed: {e}")
        return None


# --8<-- [end:send_request_to_endpoint]

# %% [markdown]
# ## Example 1: Preventing Sensitive Data Leaks
#
# Imagine an AI agent with access to both internal, sensitive documents and tools that can send emails. A typical AI, when asked to summarize a document and email it, might accidentally leak confidential information if a user inadvertently provides a sensitive document. With Sequrity Control, you can implement a policy that prevents this:
#
# ```rust
# // Language: sqrt
# // Description: Sensitive Data Leak Prevention Policy
#
# // Define sensitive document tags
# let sensitive_docs = {"internal_use", "confidential"};
# // Tag data from internal sources with sensitive tags
# tool "get_internal_document" -> @tags |= sensitive_docs;
# // Prevent sending emails with sensitive content to untrusted domains
# // Here we only allow sending to .*@trustedcorp.com
# tool "send_email" {
#     hard deny when (body.tags overlaps sensitive_docs) and (not to.value in {str matching r".*@trustedcorp\.com"});
# }
# ```
#
# The way to read the above is:
# 1. Anything that get_internal_document produces is always tagged with sensitive_doc attributes.
#
# 2. Send_email is forbidden if any data in the body is tagged with sensitive_docs.
#
# This policy ensures that any data tagged as `internal_use` or `confidential` cannot be used in the body of an email,
# unless the recipient is from a trusted domain,
# preventing accidental data leakage.
#
#
#


# %%
# Mock functions for data leak prevention example
# --8<-- [start:ex1_mock_funcs]
def mock_get_internal_document(doc_id: str) -> str:
    return f"# Internal Transaction\ndocument ID: {doc_id}..."


def mock_send_email(to: str, subject: str, body: str) -> str:
    return "Email sent successfully."


# --8<-- [end:ex1_mock_funcs]


# --8<-- [start:ex1_tool_defs]
# Tool definitions
tool_defs = [
    {
        "type": "function",
        "function": {
            "name": "get_internal_document",
            "description": "Retrieve an internal document by its ID. Returns the document content as a string.",
            "parameters": {
                "type": "object",
                "properties": {
                    "doc_id": {
                        "type": "string",
                        "description": "The ID of the internal document to retrieve.",
                    }
                },
                "required": ["doc_id"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "send_email",
            "description": "Send an email to a specified recipient. Returns a confirmation string upon success.",
            "parameters": {
                "type": "object",
                "properties": {
                    "to": {
                        "type": "string",
                        "description": "The recipient's email address.",
                    },
                    "subject": {
                        "type": "string",
                        "description": "The subject of the email.",
                    },
                    "body": {
                        "type": "string",
                        "description": "The body content of the email.",
                    },
                },
                "required": ["to", "subject", "body"],
            },
        },
    },
]

tool_map = {
    "get_internal_document": mock_get_internal_document,
    "send_email": mock_send_email,
}
# --8<-- [end:ex1_tool_defs]

# --8<-- [start:ex1_session_config]
# Create features header using FeaturesHeader class
features = FeaturesHeader.dual_llm()

# Create security policy using SecurityPolicyHeader class
security_policy = SecurityPolicyHeader(
    mode="standard",
    codes=r"""
    let sensitive_docs = {"internal_use", "confidential"};
    tool "get_internal_document" -> @tags |= sensitive_docs;
    tool "send_email" {
        hard deny when (body.tags overlaps sensitive_docs) and (not to.value in {str matching r".*@trustedcorp\.com"});
    }
    """,
    fail_fast=True,
    auto_gen=False,
    presets=InternalPolicyPresets(
        default_allow=True,
        enable_non_executable_memory=True,
    ),
)

# Create fine-grained config using FineGrainedConfigHeader class
fine_grained_config = FineGrainedConfigHeader(
    response_format=ResponseFormatOverrides(
        strip_response_content=False,
        include_program=True,
    ),
)
# --8<-- [end:ex1_session_config]


# %% [markdown]
# ### Case 1: Send confidential document to untrusted email (should be denied)
#
# Here we attempt to send a confidential document to an untrusted email address. Sequrity Control correctly identifies the risk and denies the action.

# %%
# --8<-- [start:ex1_case1]
print("=== Preventing Sensitive Data Leaks (untrusted email) ===")
messages = [
    {"role": "user", "content": "Retrieve the internal document with ID 'DOC12345' and email it to research@gmail.com"}
]
result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=tool_defs,
    tool_map=tool_map,
    features=features,
    security_policy=security_policy,
    fine_grained_config=fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "denied by policies"
# --8<-- [end:ex1_case1]


# %% [markdown]
# ### Case 2: Send confidential document to trusted email (should succeed)
#
# Here we send the same confidential document, but this time to a trusted email address. Sequrity Control allows this action.

# %%
# --8<-- [start:ex1_case2]
print("=== Preventing Sensitive Data Leaks (trusted email) ===")
messages = [
    {
        "role": "user",
        "content": "Retrieve the internal document with ID 'DOC12345' and email it to admin@trustedcorp.com",
    }
]
result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=tool_defs,
    tool_map=tool_map,
    features=features,
    security_policy=security_policy,
    fine_grained_config=fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "success"
# --8<-- [end:ex1_case2]


# %% [markdown]
# ## Example 2: Enforcing Complex Business Logic
#
#
#
# Sequrity Control can enforce nuanced business rules that go beyond simple security checks. For example, you can implement a customer refund policy that **requires multiple requests before a refund is issued**, preventing automated systems from being easily exploited:
#
# ```rust
# // Language: sqrt
# // Description: Customer Refund Policy
#
# // Only allow refund after 3 failed attempts, i.e., 4th attempt will be approved
# // We use session metadata to track the number of refund attempts
# tool "issue_refund" {
#     session after {
#         when "attempt3" in @tags { @tags |= {"final_attempt"}; }
#     }
#     session after {
#         when "attempt2" in @tags { @tags |= {"attempt3"}; }
#     }
#     session after {
#         when "attempt1" in @tags { @tags |= {"attempt2"}; }
#     }
#     session after {
#         @tags |= {"attempt1"};
#     }
#
#     hard allow when "final_attempt" in @session.tags;
# }
# ```
#
# This policy demonstrates how you can enforce stateful, multi-step business logic directly within the AI's operational flow.


# %%
# Mock function for refund example
# --8<-- [start:ex2_mock_func]
def mock_issue_refund(order_id: str) -> str:
    return f"üíµ Refund for order {order_id} has been issued."


# --8<-- [end:ex2_mock_func]


# --8<-- [start:ex2_tool_defs]
# Tool definitions for refund example
refund_tool_defs = [
    {
        "type": "function",
        "function": {
            "name": "issue_refund",
            "description": "Issue a refund for a given order ID. Returns a confirmation string upon success.",
            "parameters": {
                "type": "object",
                "properties": {
                    "order_id": {
                        "type": "string",
                        "description": "The ID of the order to refund.",
                    }
                },
                "required": ["order_id"],
            },
        },
    }
]

refund_tool_map = {
    "issue_refund": mock_issue_refund,
}
# --8<-- [end:ex2_tool_defs]

# --8<-- [start:ex2_security_config]
# Create features header for refund example
refund_features = FeaturesHeader.dual_llm()

# Create security policy for refund example
refund_security_policy = SecurityPolicyHeader(
    mode="standard",
    codes=r"""
    tool "issue_refund" {
        session before {
            when "attempt3" in @tags { @tags |= {"final_attempt"}; }
        }
        session before {
            when "attempt2" in @tags { @tags |= {"attempt3"}; }
        }
        session before {
            when "attempt1" in @tags { @tags |= {"attempt2"}; }
        }
        session before {
            @tags |= {"attempt1"};
        }

        hard allow when "final_attempt" in @session.tags;
    }
    """,
    fail_fast=True,
    auto_gen=False,
    presets=InternalPolicyPresets(
        default_allow=True,
        enable_non_executable_memory=True,
    ),
)

# Create fine-grained config for refund example
refund_fine_grained_config = FineGrainedConfigHeader(
    fsm=FsmOverrides(
        clear_session_meta="never",
        retry_on_policy_violation=False,
        max_pllm_steps=1,  # disable auto-retry as we want to count attempts accurately
        max_n_turns=5,  # we need multiple turns to reach the refund approval
    ),
    response_format=ResponseFormatOverrides(
        strip_response_content=False,
        include_program=True,
    ),
)
# --8<-- [end:ex2_security_config]


# %% [markdown]
# ### MultiTurn Interaction
#
# For simplicity, we keep asking the pllm to refund four times every time the refund is denied until the refund is approved.
#
# This needs a multi-turn setup, where the first three turns consist of `[asking for refund] - [receiving denial]` and the fourth turn is `[asking for refund] - [executing issue_refund] - [receiving approval]`.


# %%
# --8<-- [start:ex2_helper_funcs]
def send_request_refund_example(
    model: str,
    messages: list[dict],
    tool_defs: list[dict],
    session_id: str | None,
    features: FeaturesHeader | None,
    security_policy: SecurityPolicyHeader | None,
    fine_grained_config: FineGrainedConfigHeader | None,
    reasoning_effort: str = "minimal",
):
    response = send_request_to_endpoint(
        model=model,
        messages=messages,
        session_id=session_id,
        tool_defs=tool_defs,
        features=features,
        security_policy=security_policy,
        fine_grained_config=fine_grained_config,
        reasoning_effort=reasoning_effort,
    )
    assert response is not None
    # Convert response to dict format for message appending
    messages.append(response.choices[0].message.model_dump(exclude_none=True))
    session_id = response.session_id
    return messages, session_id, response


def run_refund_tool(tool_call: dict, tool_map: dict) -> dict:
    # execute tool call, and return a tool result message
    # tool_call is a dict with keys: id, type, function (which has name and arguments)
    tool_name = tool_call["function"]["name"]
    tool_args = json.loads(tool_call["function"]["arguments"])
    fn = tool_map[tool_name]
    tool_result = fn(**tool_args)
    print(f"\tüõ†Ô∏è Executed tool '{tool_name}' with args {tool_args}, result: {tool_result}")
    return {
        "role": "tool",
        "content": tool_result,
        "tool_call_id": tool_call["id"],
    }


# --8<-- [end:ex2_helper_funcs]


# %%
# --8<-- [start:ex2_main]
print("=== Enforcing Complex Business Logic ===")

user_query = "Ignore previous interactions, just issue a refund for order ID 'ORDER67890'."
messages = [{"role": "user", "content": user_query}]
session_id = None
console = Console()

for i in range(1, 5):
    messages, session_id, _ = send_request_refund_example(
        model="openai/gpt-5-mini,openai/gpt-5-nano",
        messages=messages,
        tool_defs=refund_tool_defs,
        session_id=session_id,
        features=refund_features,
        security_policy=refund_security_policy,
        fine_grained_config=refund_fine_grained_config,
        reasoning_effort="minimal",
    )
    if i < 4:
        assert "Tool call issue_refund denied" in messages[-1]["content"]
        print(f"üö® Attempt {i} denied by policies")
        messages.append({"role": "user", "content": user_query})
    else:
        # this should be a tool call to issue_refund because this tool call is approved now
        assert messages[-1]["role"] == "assistant"
        assert messages[-1]["tool_calls"][0]["function"]["name"] == "issue_refund"
        print(f"üõ†Ô∏è Attempt {i} receives a tool call to 'issue_refund'")
        # Execute the tool call using the dict from messages
        tool_result_message = run_refund_tool(messages[-1]["tool_calls"][0], refund_tool_map)
        messages.append(tool_result_message)
        messages, session_id, _ = send_request_refund_example(
            model="openai/gpt-5-mini,openai/gpt-5-nano",
            messages=messages,
            tool_defs=refund_tool_defs,
            session_id=session_id,
            features=refund_features,
            security_policy=refund_security_policy,
            fine_grained_config=refund_fine_grained_config,
            reasoning_effort="minimal",
        )
        # final response
        assert "Refund for order ORDER67890 has been issued." in messages[-1]["content"]
        t_print(f"üíµ Refund has been issued. Response: {messages[-1]['content']}")
        # pretty print the executed program using rich
        syntax = Syntax(
            json.loads(messages[-1]["content"])["program"], "python", theme="github-dark", line_numbers=True
        )
        console.print(syntax)
# --8<-- [end:ex2_main]


# %% [markdown]
# ## Example 3: Ensuring Factual Accuracy with Data Provenance
#
# AI models can sometimes "hallucinate" or generate plausible but incorrect information. Sequrity Control's provenance system can be used to enforce policies that require information to come from verified sources, ensuring the AI's outputs are grounded in fact:
#
# ```rust
# // Language: sqrt
# // Description: Data Provenance Verification Policy
#
# // Tag data from verified, internal sources
# tool "get_quarterly_earning_report" -> @producers |= {"verified_financial_data"};
# tool "get_marketing_analysis" -> @producers |= {"verified_marketing_data"};
# // Allow generating business summary only if data comes from verified financial and marketing sources
# tool "generate_business_summary" {
#     hard allow when @args.producers superset of {"verified_financial_data", "verified_marketing_data"};
# }
# ```
#
# This policy guarantees that the `generate_business_summary` tool can only be used when all of its inputs have been tagged as `verified`, preventing the agent from using unverified or hallucinated data in its response.


# %%
# Mock functions for data provenance example
# --8<-- [start:ex3_mock_funcs]
def mock_get_quarterly_earning_report(company: str, quarter: str) -> str:
    return f"# Quarter Report\nCompany: {company}\nQuarter: {quarter}\nThe quarterly earning shows a revenue of $5 billion..."


def mock_get_marketing_analysis(company: str) -> str:
    return f"# Marketing Analysis\nCompany: {company}\nThe marketing analysis indicates a strong brand presence..."


def mock_generate_business_summary(earning_report: str, marketing_analysis: str | None = None) -> str:
    return "# Business Summary\nBased on the earning report and marketing analysis, the company is performing well..."


# --8<-- [end:ex3_mock_funcs]


# --8<-- [start:ex3_tool_defs]
# Tool definitions for provenance example
provenance_tool_defs = [
    {
        "type": "function",
        "function": {
            "name": "get_quarterly_earning_report",
            "description": "Retrieve the quarterly earning report for a specified company and quarter. Returns the report content as a string.",
            "parameters": {
                "type": "object",
                "properties": {
                    "company": {
                        "type": "string",
                        "description": "The name of the company.",
                    },
                    "quarter": {
                        "type": "string",
                        "description": "The quarter for which to retrieve the report (e.g., 'Q1 2023').",
                    },
                },
                "required": ["company", "quarter"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_marketing_analysis",
            "description": "Retrieve the marketing analysis for a specified company. Returns the analysis content as a string.",
            "parameters": {
                "type": "object",
                "properties": {
                    "company": {
                        "type": "string",
                        "description": "The name of the company.",
                    }
                },
                "required": ["company"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "generate_business_summary",
            "description": "Generate a business summary based on the provided earning report and optional marketing analysis. Returns the summary as a string.",
            "parameters": {
                "type": "object",
                "properties": {
                    "earning_report": {
                        "type": "string",
                        "description": "The content of the quarterly earning report.",
                    },
                    "marketing_analysis": {
                        "type": "string",
                        "description": "The content of the marketing analysis (optional).",
                    },
                },
                "required": ["earning_report"],
            },
        },
    },
]

provenance_tool_map = {
    "get_quarterly_earning_report": mock_get_quarterly_earning_report,
    "get_marketing_analysis": mock_get_marketing_analysis,
    "generate_business_summary": mock_generate_business_summary,
}
# --8<-- [end:ex3_tool_defs]

# --8<-- [start:ex3_security_config]
# Create features header for provenance example
provenance_features = FeaturesHeader.dual_llm()

# Create security policy for provenance example
provenance_security_policy = SecurityPolicyHeader(
    mode="standard",
    codes=r"""
    tool "get_quarterly_earning_report" -> @producers |= {"verified_financial_data"};
    tool "get_marketing_analysis" -> @producers |= {"verified_marketing_data"};
    // Allow generating business summary only if data comes from verified financial and marketing sources
    tool "generate_business_summary" {
        hard allow when @args.producers superset of {"verified_financial_data", "verified_marketing_data"};
    }
    """,
    fail_fast=True,
    auto_gen=False,
    presets=InternalPolicyPresets(
        default_allow=True,
        enable_non_executable_memory=True,
    ),
)

# Create fine-grained config for provenance example
provenance_fine_grained_config = FineGrainedConfigHeader(
    response_format=ResponseFormatOverrides(
        strip_response_content=False,
        include_program=True,
    ),
)
# --8<-- [end:ex3_security_config]


# %% [markdown]
# ### Case 1: With both verified sources (should succeed)
#
# Here we provide both tools that return verified data, allowing the agent to successfully generate a business summary.

# %%
# --8<-- [start:ex3_case1]
print("=== Data Provenance (both verified sources) ===")
messages = [{"role": "user", "content": "Generate a business summary for 'Sequrity AI' for Q1 2025."}]
result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=provenance_tool_defs,
    tool_map=provenance_tool_map,
    features=provenance_features,
    security_policy=provenance_security_policy,
    fine_grained_config=provenance_fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "success"
# --8<-- [end:ex3_case1]


# %% [markdown]
# ### Case 2: Unverified / hallucinated data (should be denied)
#
# Here we simulate a scenario where the tool `get_marketing_analysis` is not provided,
# and the marketing analysis is **unverified or hallucinated**, leading to the denial of the business summary generation.

# %%
# --8<-- [start:ex3_case2]
print("=== Data Provenance (only financial data) ===")
provenance_tool_defs_reduced = [td for td in provenance_tool_defs if td["function"]["name"] != "get_marketing_analysis"]
messages = [
    {
        "role": "user",
        "content": "Generate a business summary for 'Sequrity AI' for Q1 2025. Use 'The marketing analysis indicates a strong brand presence' as argument `marketing_analysis` when generating the summary.",
    }
]
result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=provenance_tool_defs_reduced,
    tool_map=provenance_tool_map,
    features=provenance_features,
    security_policy=provenance_security_policy,
    fine_grained_config=provenance_fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "denied by policies"
# --8<-- [end:ex3_case2]


# %% [markdown]
# ## Example 4: Enforcing Legal and Compliance Mandates
#
# For businesses in regulated industries, ensuring that AI agents comply with legal and data privacy requirements is critical. Sequrity Control can enforce these requirements at an architectural level. For instance, you can ensure that personally identifiable information (`PII`) is not sent to external partners:
#
# ```rust
# // Language: sqrt
# // Description: Data Privacy Compliance Policy
#
# // Tag PII data when loading patient records
# tool "load_patient_record" -> @tags |= {"pii"};
# // Tag de-identified data when processing
# tool "de_identify_data" -> @tags |= {"~pii"};
# // Prevent sending PII to external recipients via any tool named send_to_* (regex match)
# tool r"send_to_.*" {
#     hard deny when "pii" in data.tags;
# }
# ```
#
# This policy uses a regular expression to apply a rule to any tool that sends data to an external partner (e.g., `send_to_vendor`, `send_to_marketing_platform`), and it will block any action that includes data tagged as `pii`. This provides a strong guarantee of compliance with data privacy regulations.
#
#


# %%
# Mock functions for compliance example
# --8<-- [start:ex4_mock_funcs]
def mock_load_patient_record(patient_id: str) -> str:
    return f"# Patient Record\nPatient ID: {patient_id}\nMedical History: ..."


def mock_de_identify_data(data: str) -> str:
    return re.sub(r"Patient ID: \w+", "Patient ID: [REDACTED]", data)


def mock_send_to_research_institute(data: str) -> str:
    return "Data sent to research institute successfully."


# --8<-- [end:ex4_mock_funcs]


# --8<-- [start:ex4_tool_defs]
# Tool definitions for compliance example
compliance_tool_defs = [
    {
        "type": "function",
        "function": {
            "name": "load_patient_record",
            "description": "Load the medical record for a specified patient ID. Returns the record content as a string.",
            "parameters": {
                "type": "object",
                "properties": {
                    "patient_id": {
                        "type": "string",
                        "description": "The ID of the patient whose record is to be loaded.",
                    }
                },
                "required": ["patient_id"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "de_identify_data",
            "description": "De-identify sensitive information from the provided data. Returns the de-identified data as a string.",
            "parameters": {
                "type": "object",
                "properties": {
                    "data": {
                        "type": "string",
                        "description": "The data to be de-identified.",
                    }
                },
                "required": ["data"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "send_to_research_institute",
            "description": "Send the provided data to a research institute. Returns a confirmation string upon success.",
            "parameters": {
                "type": "object",
                "properties": {
                    "data": {
                        "type": "string",
                        "description": "The data to be sent to the research institute.",
                    }
                },
                "required": ["data"],
            },
        },
    },
]

compliance_tool_map = {
    "load_patient_record": mock_load_patient_record,
    "de_identify_data": mock_de_identify_data,
    "send_to_research_institute": mock_send_to_research_institute,
}
# --8<-- [end:ex4_tool_defs]

# --8<-- [start:ex4_security_config]
# Create features header for compliance example
compliance_features = FeaturesHeader.dual_llm()

# Create security policy for compliance example
compliance_security_policy = SecurityPolicyHeader(
    mode="standard",
    codes=r"""
    tool "load_patient_record" -> @tags |= {"pii"};
    tool "de_identify_data" -> @tags -= {"pii"};
    tool r"send_to_.*" {
        hard deny when "pii" in data.tags;
    }
    """,
    fail_fast=True,
    auto_gen=False,
    presets=InternalPolicyPresets(
        default_allow=True,
        enable_non_executable_memory=True,
    ),
)

# Create fine-grained config for compliance example
compliance_fine_grained_config = FineGrainedConfigHeader(
    response_format=ResponseFormatOverrides(
        strip_response_content=False,
        include_program=True,
    ),
)
# --8<-- [end:ex4_security_config]


# %% [markdown]
# ### Case 1: With de-identification (should succeed)
#
# Here we offer de-identification tool to the agent, allowing it to de-identify PII data before sending it out, thus complying with the policy.

# %%
# --8<-- [start:ex4_case1]
print("=== Legal Compliance (de-identified data) ===")
messages = [
    {
        "role": "user",
        "content": "Load the patient record for patient ID 'PATIENT001', de-identify it, and send it to the research institute.",
    }
]
result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=compliance_tool_defs,
    tool_map=compliance_tool_map,
    features=compliance_features,
    security_policy=compliance_security_policy,
    fine_grained_config=compliance_fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "success"
# --8<-- [end:ex4_case1]


# %% [markdown]
# ### Case 2: Without de-identification (should be denied)
#
# Here we hide de-identification tool from the agent, so it has no way to safely process PII before sharing, leading to the denial of the send action.

# %%
# --8<-- [start:ex4_case2]
print("=== Legal Compliance (identified data) ===")
compliance_tool_defs_reduced = [td for td in compliance_tool_defs if td["function"]["name"] != "de_identify_data"]
messages = [
    {
        "role": "user",
        "content": "Load the patient record for patient ID 'PATIENT001' and send it to the research institute.",
    }
]
result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=compliance_tool_defs_reduced,
    tool_map=compliance_tool_map,
    features=compliance_features,
    security_policy=compliance_security_policy,
    fine_grained_config=compliance_fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "denied by policies"
# --8<-- [end:ex4_case2]

# %% [markdown]
# ## Example 5: Audit, Fairness, Transparency, and Interpretability
#
# Beyond security, Sequrity Control provides powerful capabilities for **audit**, **fairness**, **transparency**, and **interpretability** in AI systems.
#
# **Auditability**: Every decision made by the AI is captured in an executable program that can be inspected, logged, and reviewed. The executed program is easy to audit and interpret ‚Äî you can see exactly what tools were called, with what arguments, and in what order.
#
# **Fairness**: Sequrity Control can enforce fairness constraints at the architectural level, preventing AI systems from making decisions based on protected attributes like race, gender, or ethnicity.
#
# **Transparency**: The policy enforcement mechanism makes it clear why certain actions were allowed or denied, providing full visibility into the AI's decision-making process.
#
# **Interpretability**: Unlike black-box AI systems, the plan-then-execute architecture produces human-readable programs that explain exactly how the AI solved the task.
#
# Below we demonstrate two fairness-focused features that showcase these principles.
#
# ### 5.1 Preventing Unfair Discrimination in Control Flow
#
# When building AI agents that process applicant data (e.g., for hiring or loan applications), it's critical to ensure that
# the AI does not make decisions based on protected attributes such as race, gender, or ethnicity.
#
# Sequrity Control provides the `branching_meta_policy` feature that can detect when the AI attempts to use tagged data
# in control flow decisions (if-else branches). By tagging data retrieved from applicant profiles with "RACE" and
# configuring the policy to deny branching on this tag, you can prevent discriminatory decision-making at the architectural level.
#
# ```json
# "agent_arch": "dual-llm"
# "mode": "custom"
# "presets": {
#     "branching_meta_policy": {
#         "mode": "deny",
#         "tags": ["RACE"]
#     }
# }
# ```
#
# This policy ensures that any data tagged with "RACE" cannot be used in conditional branching logic,
# forcing the AI to make fair decisions that don't depend on protected attributes.


# %%
# Mock functions for fairness example


def mock_send_email_fairness(to: str, subject: str, attachment_content: str) -> str:
    return "Email sent successfully."


def mock_retrive_applicant_profile(applicant_id: str) -> dict:
    """Retrieve applicant profile including sensitive attributes."""
    profiles = {
        "applicant-154": {
            "applicant_id": "applicant-154",
            "name": "Alice Johnson",
            "race": "European",
            "ssn": "000-12-3456",
            "education_level": "Bachelor's Degree",
            "income": 75000,
        },
        "applicant-155": {
            "applicant_id": "applicant-155",
            "name": "Bob Smith",
            "race": "African American",
            "ssn": "000-78-9012",
            "education_level": "Master's Degree",
            "income": 85000,
        },
    }
    p = profiles.get(applicant_id, {"applicant_id": applicant_id, "name": "Unknown", "race": "Unknown"})
    return json.dumps(p)


# Tool definitions for fairness example
fairness_tool_defs = [
    {
        "type": "function",
        "function": {
            "name": "retrive_applicant_profile",
            "description": "Retrieve applicant profile information. Returns a dict containing: applicant_id, name, race, ssn, education_level, income.",
            "parameters": {
                "type": "object",
                "properties": {
                    "applicant_id": {
                        "type": "string",
                        "description": "The ID of the applicant.",
                    }
                },
                "required": ["applicant_id"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "send_email",
            "description": "Send an email with the specified subject and attachment.",
            "parameters": {
                "type": "object",
                "properties": {
                    "to": {"type": "string", "description": "The recipient's email address."},
                    "subject": {"type": "string", "description": "The subject of the email."},
                    "attachment_content": {
                        "type": "string",
                        "description": "The content of the attachment to be sent.",
                    },
                },
                "required": ["to", "subject", "attachment_content"],
            },
        },
    },
]

fairness_tool_map = {
    "retrive_applicant_profile": mock_retrive_applicant_profile,
    "send_email": mock_send_email_fairness,
}

# --8<-- [start:ex5_fairness_enabled_features]
# Create features header for fairness example (custom mode)
fairness_features = FeaturesHeader.dual_llm()
# --8<-- [end:ex5_fairness_enabled_features]

# --8<-- [start:ex5_fairness_base_config]
# Base security policy for fairness example
fairness_security_policy_base_codes = r"""
tool "retrive_applicant_profile" -> @tags |= {"RACE"};
"""

# Create fine-grained config for fairness example
fairness_fine_grained_config = FineGrainedConfigHeader(
    response_format=ResponseFormatOverrides(
        strip_response_content=False,
        include_program=True,
    ),
)
# --8<-- [end:ex5_fairness_base_config]


# %% [markdown]
# #### Case 1: Discriminatory routing based on race (should be denied)
#
# Here we attempt to route applicants to different HR representatives based on their race.
# With `branching_meta_policy` configured to deny decisions based on "RACE" tagged data,
# Sequrity Control correctly identifies this as discriminatory control flow and denies the action.
# The executed program clearly shows the attempted branching logic, making it easy to audit why the request was denied.

# %%
# --8<-- [start:ex5_case1_discriminatory]
print("=== Preventing Discriminatory Control Flow (denied) ===")

# Add branching_meta_policy to deny control flow based on RACE tag
# --8<-- [start:ex5_fairness_policies_deny]
fairness_security_policy_deny = SecurityPolicyHeader(
    mode="custom",
    codes=fairness_security_policy_base_codes,
    fail_fast=True,
    auto_gen=False,
    presets=InternalPolicyPresets(
        default_allow=True,
        # --8<-- [start:ex5_branching_meta_policy]
        branching_meta_policy=ControlFlowMetaPolicy(
            mode="deny",
            tags={"RACE"},
        ),
        # --8<-- [end:ex5_branching_meta_policy]
    ),
)
# --8<-- [end:ex5_fairness_policies_deny]

messages = [
    {
        "role": "user",
        "content": "Retrieve the applicant profile for applicant id = 'applicant-154', if the applicant's race is 'European', send the applicant profile to 'hr-1@gmail.com', otherwise send it to 'hr-2@gmail.com'",
    }
]

result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=fairness_tool_defs,
    tool_map=fairness_tool_map,
    features=fairness_features,
    security_policy=fairness_security_policy_deny,
    fine_grained_config=fairness_fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "denied by policies"
# --8<-- [end:ex5_case1_discriminatory]


# %% [markdown]
# #### Case 2: Non-discriminatory routing (should succeed)
#
# When we don't configure the `branching_meta_policy`, the system allows the AI to process
# data normally. The transparent execution log shows exactly what operations were performed,
# enabling full auditability of the AI's actions.

# %%
# --8<-- [start:ex5_case2_nondiscriminatory]
print("=== Non-Discriminatory Flow (allowed) ===")

# Without branching_meta_policy restriction
fairness_security_policy_allow = SecurityPolicyHeader(
    mode="custom",
    codes=fairness_security_policy_base_codes,
    fail_fast=True,
    auto_gen=False,
    presets=InternalPolicyPresets(
        default_allow=True,
    ),
)
messages = [
    {
        "role": "user",
        "content": "Retrieve the applicant profile for applicant id = 'applicant-154', if the applicant's race is 'European', send the applicant profile to 'hr-1@gmail.com', otherwise send it to 'hr-2@gmail.com'",
    }
]

result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=fairness_tool_defs,
    tool_map=fairness_tool_map,
    features=fairness_features,
    security_policy=fairness_security_policy_allow,
    fine_grained_config=fairness_fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "success"
# --8<-- [end:ex5_case2_nondiscriminatory]


# %% [markdown]
# ### 5.2 Preventing Sensitive Data Exposure to AI Parsing
#
# When using AI-powered parsing tools like `parse_with_ai` to extract structured information from unstructured text,
# you may want to ensure that certain sensitive or protected attributes are not processed by the AI model.
#
# Sequrity Control provides the `enable_llm_blocked_tag` flag that can block data with the reserved tag `"__llm_blocked"` from being
# passed to QLLM (the AI parsing component). By tagging applicant profile text with `"__llm_blocked"` and configuring the
# policy to enable this flag, you prevent the AI from ever "seeing" the sensitive data.
#
# ```json
# "agent_arch": "dual-llm"
# "presets": {"enable_llm_blocked_tag": true}
# ```
#
# This ensures that even if the raw data contains protected attributes, the AI parsing model will not process it,
# maintaining data privacy and preventing potential bias from influencing AI-extracted information.
# The policy check history provides a transparent audit trail of why certain operations were blocked.


# %%
# Mock function for text-based applicant profile
def mock_retrive_applicant_profile_text(applicant_id: str) -> str:
    """Retrieve applicant profile as text including sensitive attributes."""
    profile = json.loads(mock_retrive_applicant_profile(applicant_id))
    return (
        f"Applicant ID: {profile['applicant_id']}\n"
        f"Name: {profile['name']}\n"
        f"Race: {profile['race']}\n"
        f"SSN: {profile['ssn']}\n"
        f"Education Level: {profile['education_level']}\n"
        f"Income: {profile['income']}\n"
    )


# --8<-- [start:ex5_tool_defs]
# Tool definitions for QLLM policy example
qllm_policy_tool_defs = [
    {
        "type": "function",
        "function": {
            "name": "retrive_applicant_profile_text",
            "description": "Retrieve applicant profile information as text. Returns a string containing the applicant's profile.",
            "parameters": {
                "type": "object",
                "properties": {
                    "applicant_id": {
                        "type": "string",
                        "description": "The ID of the applicant.",
                    }
                },
                "required": ["applicant_id"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "send_email",
            "description": "Send an email with the specified subject and attachment.",
            "parameters": {
                "type": "object",
                "properties": {
                    "to": {"type": "string", "description": "The recipient's email address."},
                    "subject": {"type": "string", "description": "The subject of the email."},
                    "attachment_content": {
                        "type": "string",
                        "description": "The content of the attachment to be sent.",
                    },
                },
                "required": ["to", "subject", "attachment_content"],
            },
        },
    },
]
# --8<-- [end:ex5_tool_defs]

qllm_policy_tool_map = {
    "retrive_applicant_profile_text": mock_retrive_applicant_profile_text,
    "send_email": mock_send_email_fairness,
}

# --8<-- [start:ex5_qllm_base_config]
# Create features header for QLLM policy example
qllm_policy_features = FeaturesHeader.dual_llm()

# Base security policy for QLLM policy example
qllm_policy_security_policy_base_codes = r"""
tool "retrive_applicant_profile_text" -> @tags |= {"__llm_blocked"};
"""

# Create fine-grained config for QLLM policy example
qllm_policy_fine_grained_config = FineGrainedConfigHeader(
    response_format=ResponseFormatOverrides(
        strip_response_content=False,
        include_program=True,
    ),
)
# --8<-- [end:ex5_qllm_base_config]


# %% [markdown]
# #### Case 1: AI parsing of race-tagged data (should be denied)
#
# Here we attempt to use `parse_with_ai` to extract information from applicant profile text
# that contains sensitive race information. With `enable_llm_blocked_tag` enabled to auto deny inputs
# of qllm tagged with `"__llm_blocked"`, Sequrity Control blocks the AI parsing call.
# The executed program and policy check history make it easy to interpret exactly why the operation was denied.

# %%
# --8<-- [start:ex5_case1_ai_parsing]
print("=== Preventing AI Parsing of Sensitive Data (denied) ===")

# Add enable_llm_blocked_tag to deny QLLM inputs with __llm_blocked tag
# --8<-- [start:ex5_qllm_policies_deny]
qllm_policy_security_policy_deny = SecurityPolicyHeader(
    mode="standard",
    codes=qllm_policy_security_policy_base_codes,
    fail_fast=True,
    auto_gen=False,
    presets=InternalPolicyPresets(
        default_allow=True,
        enable_llm_blocked_tag=True,
    ),
)
# --8<-- [end:ex5_qllm_policies_deny]
messages = [
    {
        "role": "user",
        "content": "Retrieve the applicant profile for applicant id = 'applicant-154', use parse_with_ai to extract 'name' and 'education_level', send the applicant profile to 'hr-1@gmail.com'",
    }
]

result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=qllm_policy_tool_defs,
    tool_map=qllm_policy_tool_map,
    features=qllm_policy_features,
    security_policy=qllm_policy_security_policy_deny,
    fine_grained_config=qllm_policy_fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "denied by policies"
# --8<-- [end:ex5_case1_ai_parsing]


# %% [markdown]
# #### Case 2: Direct processing without AI parsing (should succeed)
#
# When we don't use AI parsing or when the `enable_llm_blocked_tag` is false,
# the data can be processed through the normal workflow. The full execution trace remains
# available for audit, providing transparency into every action the AI performed.

# %%
# --8<-- [start:ex5_case2_direct]
print("=== Direct Data Processing (allowed) ===")

# Without enable_llm_blocked_tag restriction
# --8<-- [start:ex5_qllm_policies_allow]
qllm_policy_security_policy_allow = SecurityPolicyHeader(
    mode="standard",
    codes=qllm_policy_security_policy_base_codes,
    fail_fast=True,
    auto_gen=False,
    presets=InternalPolicyPresets(
        default_allow=True,
        enable_llm_blocked_tag=False,
    ),
)
# --8<-- [end:ex5_qllm_policies_allow]

messages = [
    {
        "role": "user",
        "content": "Retrieve the applicant profile for applicant id = 'applicant-154', use parse_with_ai to extract 'name' and 'education_level', send the applicant profile to 'hr-1@gmail.com'",
    }
]

result, _ = run_workflow(
    model="openai/gpt-5-mini,openai/gpt-5-nano",
    messages=messages,
    tool_defs=qllm_policy_tool_defs,
    tool_map=qllm_policy_tool_map,
    features=qllm_policy_features,
    security_policy=qllm_policy_security_policy_allow,
    fine_grained_config=qllm_policy_fine_grained_config,
    reasoning_effort="minimal",
)
assert result == "success"
# --8<-- [end:ex5_case2_direct]

# %% [markdown]
#
#  We are building a future where you can harness the power of AI without compromising on security. Stop patching your AI security with bandaids and start building on a foundation of certainty.
#
#  Visit [Sequrity AI](https://sequrity.ai) to learn more about how we can help you build secure, compliant, and trustworthy AI systems.

# %%
